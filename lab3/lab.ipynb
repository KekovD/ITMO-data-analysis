{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Тематика публикаций: Computation and Language - LLM, машинный перевод, анализ тональности, распознавание речи и тд.",
   "id": "3fccf1773b538538"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Извлечение данных с arXiv API",
   "id": "dd631f07f105c3fc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-30T20:53:34.581889Z",
     "start_time": "2025-11-30T20:53:28.258063Z"
    }
   },
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_arxiv_papers(query=\"cat:cs.CL\", max_results=300):\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate # По дате загрузки\n",
    "    )\n",
    "\n",
    "    papers_data = []\n",
    "    \n",
    "    print(f\"Загрузка {max_results} статей...\")\n",
    "    \n",
    "    results = client.results(search)\n",
    "    for result in results:\n",
    "        papers_data.append({\n",
    "            \"title\": result.title,\n",
    "            \"authors\": \", \".join([author.name for author in result.authors]),\n",
    "            \"summary\": result.summary.replace(\"\\n\", \" \"),\n",
    "            \"url\": result.entry_id\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(papers_data)\n",
    "    print(f\"Загружено {len(df)} статей.\")\n",
    "    return df\n",
    "\n",
    "df_papers = fetch_arxiv_papers()\n",
    "\n",
    "df_papers.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка 300 статей...\n",
      "Загружено 300 статей.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  Revisiting Generalization Across Difficulty Le...   \n",
       "1  ToolOrchestra: Elevating Intelligence via Effi...   \n",
       "2  G$^2$VLM: Geometry Grounded Vision Language Mo...   \n",
       "3  Matrix: Peer-to-Peer Multi-Agent Synthetic Dat...   \n",
       "4  The author is dead, but what if they never liv...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Yeganeh Kordi, Nihal V. Nayak, Max Zuo, Ilana ...   \n",
       "1  Hongjin Su, Shizhe Diao, Ximing Lu, Mingjie Li...   \n",
       "2  Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran,...   \n",
       "3  Dong Wang, Yang Li, Ansong Ni, Ching-Feng Yeh,...   \n",
       "4  Anna Marklová, Ondřej Vinš, Martina Vokáčová, ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We investigate how well large language models ...   \n",
       "1  Large language models are powerful generalists...   \n",
       "2  Vision-Language Models (VLMs) still lack robus...   \n",
       "3  Synthetic data has become increasingly importa...   \n",
       "4  Large language models are increasingly capable...   \n",
       "\n",
       "                                 url  \n",
       "0  http://arxiv.org/abs/2511.21692v1  \n",
       "1  http://arxiv.org/abs/2511.21689v1  \n",
       "2  http://arxiv.org/abs/2511.21688v1  \n",
       "3  http://arxiv.org/abs/2511.21686v1  \n",
       "4  http://arxiv.org/abs/2511.21629v1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revisiting Generalization Across Difficulty Le...</td>\n",
       "      <td>Yeganeh Kordi, Nihal V. Nayak, Max Zuo, Ilana ...</td>\n",
       "      <td>We investigate how well large language models ...</td>\n",
       "      <td>http://arxiv.org/abs/2511.21692v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ToolOrchestra: Elevating Intelligence via Effi...</td>\n",
       "      <td>Hongjin Su, Shizhe Diao, Ximing Lu, Mingjie Li...</td>\n",
       "      <td>Large language models are powerful generalists...</td>\n",
       "      <td>http://arxiv.org/abs/2511.21689v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G$^2$VLM: Geometry Grounded Vision Language Mo...</td>\n",
       "      <td>Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran,...</td>\n",
       "      <td>Vision-Language Models (VLMs) still lack robus...</td>\n",
       "      <td>http://arxiv.org/abs/2511.21688v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matrix: Peer-to-Peer Multi-Agent Synthetic Dat...</td>\n",
       "      <td>Dong Wang, Yang Li, Ansong Ni, Ching-Feng Yeh,...</td>\n",
       "      <td>Synthetic data has become increasingly importa...</td>\n",
       "      <td>http://arxiv.org/abs/2511.21686v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The author is dead, but what if they never liv...</td>\n",
       "      <td>Anna Marklová, Ondřej Vinš, Martina Vokáčová, ...</td>\n",
       "      <td>Large language models are increasingly capable...</td>\n",
       "      <td>http://arxiv.org/abs/2511.21629v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Разбиение на чанки ",
   "id": "20c45899e0fc616d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:53:40.891686Z",
     "start_time": "2025-11-30T20:53:40.873680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_documents_and_chunks(df):\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        content = f\"Title: {row['title']}\\nAuthors: {row['authors']}\\nAbstract: {row['summary']}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": row[\"url\"], \"title\": row[\"title\"]}))\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2500,\n",
    "        chunk_overlap=250,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Исходных документов: {len(documents)}\")\n",
    "    print(f\"Количество чанков: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "chunks = create_documents_and_chunks(df_papers)\n",
    "\n",
    "print(f\"\\nПример чанка:\\n{chunks[0].page_content[:500]}...\")"
   ],
   "id": "d0ac156f688a640",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходных документов: 300\n",
      "Количество чанков: 300\n",
      "\n",
      "Пример чанка:\n",
      "Title: Revisiting Generalization Across Difficulty Levels: It's Not So Easy\n",
      "Authors: Yeganeh Kordi, Nihal V. Nayak, Max Zuo, Ilana Nguyen, Stephen H. Bach\n",
      "Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address t...\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Векторизация текста ",
   "id": "cec7bc8ba376a5b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:47:42.501696Z",
     "start_time": "2025-11-30T21:47:35.441414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ],
   "id": "c3eadc170dafcea9",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FAISS (Facebook AI Similarity Search) - библиотека для поиска похожих векторов",
   "id": "f1b859ffc9a4fba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Интеграция с LLM ",
   "id": "222480c51ba33bc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:47:53.016710Z",
     "start_time": "2025-11-30T21:47:44.102225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    model_kwargs={\"max_input_length\": 512},\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"You are a helpful research assistant specializing in Computational Linguistics.\n",
    "Use the following pieces of retrieved context to answer the user's question. \n",
    "If you don't know the answer based on the context, just say that you don't know based on the available papers.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer concisely and specifically citing the title of the paper if possible:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "4e4e95e7555cbd31",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Обработка опечаток",
   "id": "e07424d5704213f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:47:54.550740Z",
     "start_time": "2025-11-30T21:47:54.535112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "corpus_words = set()\n",
    "if 'title' in df_papers.columns:\n",
    "    for title in df_papers['title']:\n",
    "        words = str(title).lower().split()\n",
    "        corpus_words.update(words)\n",
    "    \n",
    "corpus_list = list(corpus_words)\n",
    "\n",
    "print(f\"Размер словаря для коррекции опечаток: {len(corpus_list)} слов\")\n",
    "\n",
    "def correct_query_typos(query, threshold=85):\n",
    "    if not query or not corpus_list:\n",
    "        return query\n",
    "\n",
    "    words = query.split()\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) < 4 or word.isdigit():\n",
    "            corrected_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        match = process.extractOne(word.lower(), corpus_list, scorer=fuzz.WRatio)\n",
    "        \n",
    "        if match:\n",
    "            found_word, score, _ = match\n",
    "            if score >= threshold:\n",
    "                corrected_words.append(found_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "            \n",
    "    corrected_query = \" \".join(corrected_words)\n",
    "    \n",
    "    if corrected_query.lower() != query.lower():\n",
    "        print(f\"Обнаружена опечатка/замена: '{query}' -> '{corrected_query}'\")\n",
    "    \n",
    "    return corrected_query"
   ],
   "id": "d66ee53d15955e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря для коррекции опечаток: 1452 слов\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Тестирование на 5 примерах",
   "id": "606e960d742440bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:48:46.477994Z",
     "start_time": "2025-11-30T21:47:57.206286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "\n",
    "def compare_results(query, show_correction=True):\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        final_query = correct_query_typos(query) if show_correction else query\n",
    "    except:\n",
    "        final_query = query\n",
    "        \n",
    "    print(f\"USER QUERY: {final_query}\")\n",
    "    \n",
    "    print(\"\\n--- [RAG Answer] ---\")\n",
    "    try:\n",
    "        rag_response = rag_chain.invoke(final_query)\n",
    "        print(textwrap.fill(rag_response, width=80))\n",
    "    except Exception as e:\n",
    "        print(f\"Error RAG: {e}\")\n",
    "    \n",
    "    print(\"\\n--- [Pure LLM (No Context)] ---\")\n",
    "    try:\n",
    "        pure_prompt = f\"Question: {final_query}\\nAnswer:\"\n",
    "        pure_response = llm.invoke(pure_prompt)\n",
    "        print(textwrap.fill(pure_response, width=80))\n",
    "    except Exception as e:\n",
    "        print(f\"Error Pure LLM: {e}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"TEST 1: General Trend Extraction\")\n",
    "compare_results(\"What are the main topics discussed in these papers?\")\n",
    "\n",
    "print(\"TEST 2: Typo Handling\")\n",
    "compare_results(\"How are large languge models optimized?\")\n",
    "\n",
    "print(\"TEST 3: Specific Task Inquiry\")\n",
    "compare_results(\"Are there any papers about emotion recognition or sentiment analysis?\")\n",
    "\n",
    "print(\"TEST 4: Methodology Extraction\")\n",
    "compare_results(\"What datasets are used for evaluation in the retrieved papers?\")\n",
    "\n",
    "print(\"TEST 5: Specific Architecture Search\")\n",
    "compare_results(\"Is there any mention of reasoning capabilities in LLMs?\")"
   ],
   "id": "ab42476da934f41e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: General Trend Extraction\n",
      "================================================================================\n",
      "Обнаружена опечатка/замена: 'What are the main topics discussed in these papers?' -> 'what are the domain-adaptive topic is in the papers'\n",
      "USER QUERY: what are the domain-adaptive topic is in the papers\n",
      "\n",
      "--- [RAG Answer] ---\n",
      "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning\n",
      "\n",
      "--- [Pure LLM (No Context)] ---\n",
      "Domain-Adaptive Topic Models\n",
      "================================================================================\n",
      "\n",
      "TEST 2: Typo Handling\n",
      "================================================================================\n",
      "Обнаружена опечатка/замена: 'How are large languge models optimized?' -> 'How are large language models optimized?'\n",
      "USER QUERY: How are large language models optimized?\n",
      "\n",
      "--- [RAG Answer] ---\n",
      "Abstract: We introduce a new tokenizer for language models that minimizes the\n",
      "average tokens per character, thereby reducing the number of tokens needed to\n",
      "represent text during training and to generate text during inference. Our\n",
      "method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by\n",
      "casting a length-weighted objective maximization as a graph partitioning problem\n",
      "and developing a greedy approximation algorithm. On FineWeb and diverse domains,\n",
      "it yields 14--18% fewer tokens than Byte Pair Encoding (BPE) across vocabulary\n",
      "sizes from 10K to 50K, and the reduction is 13.0% when the size is 64K. Training\n",
      "GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each\n",
      "shows 18.5%, 17.2%, and 13.7% lower inference latency, together with a 16%\n",
      "throughput gain at 124M, while consistently improving on downstream tasks\n",
      "including reducing LAMBADA perplexity by 11.7% and enhancing HellaSwag accuracy\n",
      "by 4.3%. These results demonstrate that optimizing for average token length,\n",
      "rather than frequency alone, offers an effective approach to more efficient\n",
      "language modeling without sacrificing -- and often improving -- downstream\n",
      "performance. The tokenizer is compatible with production systems and reduces\n",
      "embedding and KV-cache memory by 18% at inference.\n",
      "\n",
      "--- [Pure LLM (No Context)] ---\n",
      "by combining a large number of languages into a single model\n",
      "================================================================================\n",
      "\n",
      "TEST 3: Specific Task Inquiry\n",
      "================================================================================\n",
      "Обнаружена опечатка/замена: 'Are there any papers about emotion recognition or sentiment analysis?' -> 'Are the any papers about emotion recognition or sentiment analysis'\n",
      "USER QUERY: Are the any papers about emotion recognition or sentiment analysis\n",
      "\n",
      "--- [RAG Answer] ---\n",
      "Annotators inter-annotator agreement on the sentiment tagged Sinhala song\n",
      "comments extracted from YouTube.\n",
      "\n",
      "--- [Pure LLM (No Context)] ---\n",
      "yes\n",
      "================================================================================\n",
      "\n",
      "TEST 4: Methodology Extraction\n",
      "================================================================================\n",
      "Обнаружена опечатка/замена: 'What datasets are used for evaluation in the retrieved papers?' -> 'what datasets are confused for evaluation in the retrieved papers'\n",
      "USER QUERY: what datasets are confused for evaluation in the retrieved papers\n",
      "\n",
      "--- [RAG Answer] ---\n",
      "Abstract: The Arctic-Extract Technical Report is a Technical Report prepared by\n",
      "the National Oceanic and Atmospheric Administration (NOAA) for the United States\n",
      "Geological Survey (USGS).\n",
      "\n",
      "--- [Pure LLM (No Context)] ---\n",
      "ESA\n",
      "================================================================================\n",
      "\n",
      "TEST 5: Specific Architecture Search\n",
      "================================================================================\n",
      "Обнаружена опечатка/замена: 'Is there any mention of reasoning capabilities in LLMs?' -> 'Is the any on of reasoning capabilities in llm'\n",
      "USER QUERY: Is the any on of reasoning capabilities in llm\n",
      "\n",
      "--- [RAG Answer] ---\n",
      "LLMs trained on reasoning traces generated by DeepSeek-R1 and OpenAI's gpt-oss.\n",
      "\n",
      "--- [Pure LLM (No Context)] ---\n",
      "logical reasoning\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Как видно из сравнения ответов, RAG выдает более полные и конкретные ответы, тогда как \"чистая\" LLM выдает общие фразы (например в тесте 3 и 5).  \n",
    "С RAG модель способна цитировать статьи (тест 2).\n",
    "\n",
    "*Работа модуля исправления опечаток*\n",
    "- В Тесте 2 система исправила languge -> language, что позволило найти нужную статью.\n",
    "- Проблема: Словарь для коррекции был собран только из заголовков статей. Из-за этого в словаре не оказалось простых слов (main, used).\n",
    "    - В Тесте 1 слово main (главные) ошибочно заменилось на domain-adaptive, так как это было самое похожее слово из заголовков.\n",
    "    - В Тесте 4 слово used (используемые) заменилось на confused (спутанные).  \n",
    "  \n",
    "=> Механизм работает, но словарь нужно расширить обычным английским лексиконом, чтобы не искажать смысл простых запросов.\n",
    "\n",
    "*Итог:*\n",
    "1. RAG превосходит чистый LLM для задач, требующих точных фактов и работы с конкретной документацией.\n",
    "2. Основная проблема RAG — это ошибки на этапе поиска (retrieval), которые приводят к генерации неверных ответов на основе нерелевантного контекста.\n",
    "3. RAG-система устойчива к опечаткам и плохой формулировке запросов.\n",
    "4. Чистый LLM без контекста непригоден для задач, требующих работы с конкретными данными, и выдает лишь общие, предсказуемые ответы.\n"
   ],
   "id": "b2de91c78795f54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
